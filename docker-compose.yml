# Optimized Docker Compose for IBKR Trading WebUI
# Fast startup with parallel service initialization and resource optimization
#
# IMPORTANT: This application requires a .env file with DATABASE_URL configured
# See DATABASE_SETUP.md for detailed setup instructions
#
# Minimal .env example:
# DATABASE_URL=postgresql+psycopg2://user:password@host:port/dbname?sslmode=require
# OPENAI_API_KEY=your_key_here
# IBKR_ACCOUNT_ID=DU1234567



x-common-variables: &common-env
  DATABASE_URL: "${DATABASE_URL}"
  REDIS_URL: "redis://redis:6379/0"
  CELERY_BROKER_URL: "redis://redis:6379/0"
  CELERY_RESULT_BACKEND: "redis://redis:6379/1"
  IBKR_API_BASE_URL: "https://ibkr-gateway:5055/v1/api"
  IBKR_ACCOUNT_ID: "${IBKR_ACCOUNT_ID:-DU1234567}"
  IBKR_SSL_VERIFY: "false"
  MINIO_ENDPOINT: "minio:9000"
  MINIO_PUBLIC_ENDPOINT: "localhost:9000"
  MINIO_ACCESS_KEY: "${MINIO_ACCESS_KEY:-minioadmin}"
  MINIO_SECRET_KEY: "${MINIO_SECRET_KEY:-minioadmin}"
  OPENAI_API_KEY: "${OPENAI_API_KEY:-your_key_here}"
  OPENAI_API_BASE: "${OPENAI_API_BASE:-https://api.openai.com/v1}"
  OPENAI_MODEL: "${OPENAI_MODEL:-gpt-4-turbo-preview}"
  LLM_VISION_MODEL: "${LLM_VISION_MODEL:-gpt-4-vision-preview}"
  LLM_VISION_PROVIDER: "${LLM_VISION_PROVIDER:-openai}"
  # Market Data Cache & Debug Mode
  DEBUG_MODE: "true"
  CACHE_ENABLED: "true"
  CACHE_SYMBOLS: "NVDA,TSLA"
  CACHE_EXCHANGE: "NASDAQ"
  CACHE_TTL_HOURS: "24"
  # Airflow & MLflow
  MLFLOW_TRACKING_URI: "http://mlflow-server:5500"
  MLFLOW_S3_ENDPOINT_URL: "http://minio:9000"
  AWS_ACCESS_KEY_ID: "${MINIO_ACCESS_KEY:-minioadmin}"
  AWS_SECRET_ACCESS_KEY: "${MINIO_SECRET_KEY:-minioadmin}"

x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"

# Airflow common configuration
x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile.airflow
  image: ibkr-airflow:latest
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: 'Fernet key must be generated and kept secret - using development key'
    AIRFLOW__WEBSERVER__SECRET_KEY: 'ibkr-trading-webui-secret-key-dev'
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # Web server base URL configuration for log URL generation
    AIRFLOW__WEBSERVER__BASE_URL: 'http://localhost:8080'
    AIRFLOW__CORE__HOSTNAME_CALLABLE: 'socket:getfqdn'
    # Log serving configuration
    AIRFLOW__LOGGING__REMOTE_LOGGING: 'false'
    AIRFLOW__LOGGING__LOGGING_LEVEL: 'INFO'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # PostgreSQL connection for workflows
    POSTGRES_HOST: postgres
    POSTGRES_PORT: '5432'
    POSTGRES_DB: postgres
    POSTGRES_USER: postgres
    POSTGRES_PASSWORD: postgres
    # MLflow configuration
    MLFLOW_TRACKING_URI: 'http://mlflow-server:5500'
    MLFLOW_S3_ENDPOINT_URL: 'http://minio:9000'
    MLFLOW_EXPERIMENT_NAME: ${MLFLOW_EXPERIMENT_NAME:-ibkr-stock-data}
    # MinIO configuration for chart uploads
    MINIO_ENDPOINT: "minio:9000"
    MINIO_PUBLIC_ENDPOINT: "localhost:9000"
    MINIO_ACCESS_KEY: "${MINIO_ACCESS_KEY:-minioadmin}"
    MINIO_SECRET_KEY: "${MINIO_SECRET_KEY:-minioadmin}"
    MINIO_BUCKET_CHARTS: "${MINIO_BUCKET_CHARTS:-trading-charts}"
    MINIO_SECURE: "false"
    # Chart storage directory (shared volume)
    CHARTS_DIR: "/app/charts"
    # Workflow configuration
    DEBUG_MODE: ${DEBUG_MODE:-false}
    STOCK_SYMBOLS: ${STOCK_SYMBOLS:-TSLA,NVDA}
    ENVIRONMENT: ${ENVIRONMENT:-development}
    WORKFLOW_SCHEDULE: ${WORKFLOW_SCHEDULE:-}  # e.g., '0 9 * * 1-5' for 9 AM weekdays, '@daily', or leave empty for manual
    # LLM configuration
    LLM_PROVIDER: ${LLM_VISION_PROVIDER:-openai}
    LLM_API_BASE_URL: ${OPENAI_API_BASE:-https://api.openai.com/v1}
    LLM_API_KEY: ${OPENAI_API_KEY:-}
    LLM_MODEL: ${OPENAI_MODEL:-gpt-4o}
    ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
    ANTHROPIC_MODEL: ${ANTHROPIC_MODEL:-claude-3-5-sonnet-20241022}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./reference/airflow/logs:/opt/airflow/logs
    - ./reference/airflow/config:/opt/airflow/config
    - ./reference/airflow/plugins:/opt/airflow/plugins
    - ./reference/airflow/models:/app/models
    - ./reference/airflow/config.yaml:/app/config.yaml
    - ./.env:/app/.env
    - charts_data:/app/charts:rw  # Shared volume for chart files with read-write permissions
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on: &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy
  networks:
    - trading-network
  logging: *default-logging

services:
  # Redis - Message Broker and Cache (starts first, fastest startup)
  redis:
    image: redis:7-alpine
    container_name: ibkr-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 2s
      retries: 3
      start_period: 5s
    sysctls:
      - net.core.somaxconn=1024

  # PostgreSQL - Local Database for Airflow and MLflow
  postgres:
    image: postgres:15
    container_name: ibkr-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-databases.sh:/docker-entrypoint-initdb.d/init-multiple-dbs.sh
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.5'

  # MinIO - S3-compatible Object Storage (starts in parallel with Redis)
  minio:
    image: minio/minio:latest
    container_name: ibkr-minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: "${MINIO_ACCESS_KEY:-minioadmin}"
      MINIO_ROOT_PASSWORD: "${MINIO_SECRET_KEY:-minioadmin}"
    ports:
      - "9000:9000"  # API
      - "9001:9001"  # Console
    volumes:
      - minio_data:/data
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s

  # IBKR Gateway - Interactive Brokers Client Portal Gateway (starts in parallel)
  ibkr-gateway:
    build:
      context: .
      dockerfile: Dockerfile
      cache_from:
        - ibkr-gateway:latest
    image: ibkr-gateway:latest
    container_name: ibkr-gateway
    environment:
      IBKR_ACCOUNT_ID: "${IBKR_ACCOUNT_ID:-DU1234567}"
    ports:
      - "5055:5055"  # IBKR Gateway API
      - "5056:5056"  # IBKR Gateway Web Interface
    volumes:
      - ./conf.yaml:/app/gateway/root/conf.yaml:ro
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-k", "-f", "https://localhost:5055/v1/api/tickle"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # FastAPI Backend (waits for Redis and MinIO to be healthy)
  # Updated to use 'ta' library instead of 'talib' for technical analysis
  backend:
    build:
      context: .
      dockerfile: docker/Dockerfile.backend
      cache_from:
        - ibkr-backend:latest
    image: ibkr-backend:latest
    container_name: ibkr-backend
    environment: *common-env
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app/backend:ro
      - ./frontend:/app/frontend:ro
      - ./logs:/app/logs
      - charts_data:/app/charts:rw  # Shared volume for chart files with read-write permissions
    depends_on:
      redis:
        condition: service_healthy
      minio:
        condition: service_healthy
      ibkr-gateway:
        condition: service_started
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    command: ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "1"]

  # Celery Worker - Background Task Processing (optimized for parallel startup)
  celery-worker:
    image: ibkr-backend:latest
    container_name: ibkr-celery-worker
    environment: *common-env
    volumes:
      - ./backend:/app/backend:ro
      - ./logs:/app/logs
    depends_on:
      redis:
        condition: service_healthy
      backend:
        condition: service_started
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    command: ["celery", "-A", "backend.celery_app", "worker", "--loglevel=info", "--concurrency=2", "--prefetch-multiplier=1"]

  # Celery Beat - Task Scheduler (lightweight, starts quickly)
  celery-beat:
    image: ibkr-backend:latest
    container_name: ibkr-celery-beat
    environment: *common-env
    volumes:
      - ./backend:/app/backend:ro
      - ./logs:/app/logs
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    command: ["celery", "-A", "backend.celery_app", "beat", "--loglevel=info"]

  # Flower - Celery Monitoring (optional, minimal resources)
  flower:
    image: ibkr-backend:latest
    container_name: ibkr-flower
    environment:
      CELERY_BROKER_URL: "redis://redis:6379/0"
      CELERY_RESULT_BACKEND: "redis://redis:6379/1"
    ports:
      - "5555:5555"
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5555"]
      interval: 30s
      timeout: 5s
      retries: 2
      start_period: 15s
    command: ["celery", "-A", "backend.celery_app", "flower", "--port=5555"]

  # MinIO Client - Creates required buckets for MLflow
  mc:
    image: minio/mc
    container_name: ibkr-mc
    depends_on:
      minio:
        condition: service_healthy
    environment:
      AWS_ACCESS_KEY_ID: "${MINIO_ACCESS_KEY:-minioadmin}"
      AWS_SECRET_ACCESS_KEY: "${MINIO_SECRET_KEY:-minioadmin}"
    entrypoint: >
      /bin/sh -c "
      /scripts/wait-for-it.sh minio:9000 &&
      /usr/bin/mc alias set minio http://minio:9000 ${MINIO_ACCESS_KEY:-minioadmin} ${MINIO_SECRET_KEY:-minioadmin} &&
      /usr/bin/mc mb minio/mlflow || true;
      exit 0;
      "
    volumes:
      - ./scripts:/scripts
    networks:
      - trading-network
    logging: *default-logging

  # MLflow Server - ML Experiment Tracking and Model Registry
  mlflow-server:
    build:
      context: ./reference/airflow/mlflow
      dockerfile: Dockerfile
    image: ibkr-mlflow:latest
    container_name: ibkr-mlflow-server
    depends_on:
      mc:
        condition: service_completed_successfully
    ports:
      - "5500:5500"
    environment:
      MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      AWS_ACCESS_KEY_ID: "${MINIO_ACCESS_KEY:-minioadmin}"
      AWS_SECRET_ACCESS_KEY: "${MINIO_SECRET_KEY:-minioadmin}"
    command: mlflow server --port 5500 --host 0.0.0.0 --backend-store-uri ${MLFLOW_DATABASE_URL} --default-artifact-root s3://mlflow --allowed-hosts "*"
    networks:
      - trading-network
    restart: unless-stopped
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Airflow Init - Database initialization and volume setup
  airflow-init:
    <<: *airflow-common
    container_name: ibkr-airflow-init
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Fix permissions on shared volumes
        mkdir -p /app/charts
        chmod -R 777 /app/charts
        
        # Wait for postgres to be ready
        until pg_isready -h postgres -p 5432 -U airflow; do
          echo "Waiting for postgres..."
          sleep 2
        done
        # Initialize database
        /entrypoint airflow db init
        # Create admin user
        /entrypoint airflow users create \
          --username ${_AIRFLOW_WWW_USER_USERNAME:-airflow} \
          --password ${_AIRFLOW_WWW_USER_PASSWORD:-airflow} \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com || echo "User may already exist"
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      <<: *airflow-common-env
    networks:
      - trading-network
    restart: "no"

  # Airflow Webserver with LocalExecutor - Using official entrypoint pattern
  airflow-webserver:
    <<: *airflow-common
    container_name: ibkr-airflow-webserver
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Use official Airflow entrypoint which handles initialization properly
        # Local PostgreSQL connection is already configured in environment
        # _AIRFLOW_DB_MIGRATE handles database migration automatically
        exec /entrypoint webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-common-env
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'
    networks:
      - trading-network
    logging: *default-logging

  # Airflow Scheduler with LocalExecutor - Runs tasks in same process
  airflow-scheduler:
    <<: *airflow-common
    container_name: ibkr-airflow-scheduler
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Use official Airflow entrypoint which handles initialization properly
        # Local PostgreSQL connection is already configured in environment
        exec /entrypoint scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      <<: *airflow-common-depends-on
    environment:
      <<: *airflow-common-env
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    networks:
      - trading-network
    logging: *default-logging

  # Airflow Triggerer - Handles deferrable tasks
  airflow-triggerer:
    <<: *airflow-common
    container_name: ibkr-airflow-triggerer
    entrypoint: /bin/bash
    command:
      - -c
      - |
        # Use official Airflow entrypoint which handles initialization properly
        # Local PostgreSQL connection is already configured in environment
        exec /entrypoint triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    environment:
      <<: *airflow-common-env
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    networks:
      - trading-network
    logging: *default-logging

# Optimized volumes with better performance settings
volumes:
  redis_data:
    name: ibkr_redis_data
    driver: local
  postgres_data:
    name: ibkr_postgres_data
    driver: local
  charts_data:
    name: ibkr_charts_data
    driver: local
  minio_data:
    name: ibkr_minio_data
    driver: local

# Optimized network with custom settings for better performance
networks:
  trading-network:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: ibkr-bridge
      com.docker.network.driver.mtu: 1500
    ipam:
      config:
        - subnet: 172.25.0.0/16
